# environment the application is running
hadoop.jobtracker=http://c5master2-vh.gbif.org:8032
hdfs.namenode=hdfs://ha-nn
oozie.url=http://c5master1-vh.gbif.org:11000/oozie

# Uses oozies shared lib and the /lib folder in the workflow
oozie.use.system.libpath=true

# yarn because the staging dir is created under yarn user's folder
user.name=yarn

# location of the workflow and jars
oozie.wf.application.path=hdfs://ha-nn/maps-backfill-workflow/
oozie.libpath=hdfs://ha-nn/maps-backfill-workflow/lib/

# fine tune the spark submission
gbif.map.spark.opts=--executor-memory 64G --num-executors 40 --executor-cores 10

# the name of the file for the configuration
gbif.map.spark.conf=prod.yml

# ZK Quorum for the HBase cluster
gbif.map.zk.quorum=c5master1-vh.gbif.org:2181,c5master2-vh.gbif.org:2181,c5master3-vh.gbif.org:2181

# Source HBase occurrence table to snapshot for the input
gbif.map.sourceTable=prod_c_occurrence

# The target table is finalised as prefix_mode_timestamp (e.g. dev_maps_tiles_201707_01_1315
gbif.map.targetTablePrefix=prod_c_maps

# The target directory for the HFiles
gbif.map.targetDirectory=hdfs://ha-nn/tmp/tim_maps

# Whether to run the point or the tile backfill
gbif.map.mode=points

# The modulus controls the number of regions in the target table (sensible defaults are 5-10 for dev and 100 for prod)
gbif.map.keySaltModulus=100
